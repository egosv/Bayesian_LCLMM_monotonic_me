/*
Latent Class Linear Mixed Models
Bayesian growth mixture model
Non-increasing process
*/

model{

### MODEL
for(j in 1:n){	
	
### First examination	
   for(i in offset[j]){		
      Yerror[i] ~ dnorm(Wtrue[i],invsigma2[g[j]])
      Wtrue[i] ~ dnorm(eta[i],invtau2)
      eta[i] <- beta0 + inprod(X[i,],beta) + inprod(U[i,],lambda[g[j],]) 
###+ inprod(Z[i,],gama[j,]) 
   }

### 2nd,3rd,â€¦,(K)th examination
   for(i in (offset[j]+1):(offset[j+1]-1)){		
      Yerror[i] ~ dnorm(Wtrue[i],invsigma2[g[j]])
      Wtrue[i] ~ dnorm(eta[i],invtau2)T(,Wtrue[i-1])
      eta[i] <- beta0 + inprod(X[i,],beta) + inprod(U[i,],lambda[g[j],]) 
###+ inprod(Z[i,],gama[j,]) 
   }

### Random effects
   gama[j,1:M] ~ dmnorm(zeros.gama, invGama0[1:M,1:M]) 
 
} 

### Latent class
for(j in 1:n){		
   g[j] ~ dcat(w[j,1:K]) ### Latent class indicator
   for(k in 1:K){
      w[j,k] <- ppi[j,k]/sum(ppi[j,1:K]) 
      log(ppi[j,k]) <- inprod(V[j,],alpha[k,])  
   }
} 

### PRIOR 
beta0 ~ dnorm(0,0.01)
### beta ~ dmnorm(zeros.beta,prior.betaB)
for(l in 1:L){
   beta[l] ~ dnorm(0,0.01)
}
for(l in 1:(K-1)){
   lambdaaux[l] ~ dnorm(0,0.01)###T(0,50) 
}
lambda[1,1] <- 0
lambda[2:K,1] <- sort(lambdaaux)
for(k in 1:K){
   for(l in 2:P){
      lambda[k,l] ~ dnorm(0,0.01) 
   } 
   invsigma2[k] ~ dgamma(2,sigma2_prior[k])
   sigma2_prior[k] ~ dgamma(0.5,10/R2w) 
} 
sigma2 <- pow(invsigma2,-1)

invtau2 ~ dgamma(2,tau2_prior)
tau2_prior ~ dgamma(0.5,10/R2w) 
tau2 <- pow(invtau2,-1)

alpha[K,1:Q] <- rep(0,Q)
for(k in 1:(K-1)){
   for(l in 1:Q){
      alpha[k,l] ~ dnorm(0,4/9)
   } 
}

invGama0[1:M,1:M] <- inverse(Gama0[1:M,1:M])
L.Gama0[1,1] <- 1.0
for(i in 2:M){
   L.Gama0[i,i] ~ dgamma(1,1)
   for(j in 1:(i-1)){
      L.Gama0[i,j] ~ dnorm(0,1)
      L.Gama0[j,i] <- 0.0
   }
}
for(i in 1:M){
   for(j in 1:M){
      Gama0[i,j] <- inprod(L.Gama0[i,1:M],L.Gama0[j,1:M])
   }
}

}

